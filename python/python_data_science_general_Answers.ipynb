{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f04ea0",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4077aab",
   "metadata": {},
   "source": [
    "## A1 – Goal: Simple Data Exploration.\n",
    "\n",
    "_Load the `tips` dataset from seaborn, report its shape, and preview the first five rows._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42873c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.load_dataset provides versioned sample data; alternatively, pd.read_csv can pull the same CSV from the seaborn GitHub repo.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "print(f\"Shape: {tips.shape}\")\n",
    "tips.head()\n",
    "# Observation: Preview confirms 244 rows across 7 tipping features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c945fc8",
   "metadata": {},
   "source": [
    "## A2.\n",
    "\n",
    "_Using the `tips` data, generate summary statistics for the numeric columns and note any skewness you observe in `total_bill`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe quickly summarizes central tendency and spread; alternatively, tips.agg({col: ['mean','median'] for col in tips.select_dtypes(include='number').columns}) gives targeted stats.\n",
    "numeric_summary = tips.select_dtypes(include=\"number\").describe().round(2)\n",
    "numeric_summary\n",
    "# Observation: total_bill is right-skewed with a longer tail toward higher bills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1ceb2",
   "metadata": {},
   "source": [
    "## A3.\n",
    "\n",
    "_Create a new `tip_pct` feature (tip divided by total bill) and report the average tip percentage by customer sex._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d023e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign keeps the dataframe chainable; alternatively, tips['tip_pct'] = tips['tip'] / tips['total_bill'] works but is less composable in pipelines.\n",
    "tips = tips.assign(tip_pct=tips[\"tip\"] / tips[\"total_bill\"])\n",
    "avg_tip_by_sex = tips.groupby(\"sex\", dropna=False)[\"tip_pct\"].mean().sort_values(ascending=False).round(3)\n",
    "avg_tip_by_sex\n",
    "# Observation: Average tip percentage runs slightly higher for female diners versus male diners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc256cf5",
   "metadata": {},
   "source": [
    "## A4.\n",
    "\n",
    "_Build a pivot table that compares average tip percentage by day of week and meal time, rounded to three decimals._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ff914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot_table handles multi-dimensional summaries; alternatively, tips.groupby(['day','time'])['tip_pct'].mean().unstack() yields the same grid.\n",
    "tip_pct_pivot = tips.pivot_table(\n",
    "    values=\"tip_pct\",\n",
    "    index=\"day\",\n",
    "    columns=\"time\",\n",
    "    aggfunc=\"mean\"\n",
    ").round(3)\n",
    "tip_pct_pivot\n",
    "# Observation: Weekend dinners tend to show the highest average tip percentages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ff241",
   "metadata": {},
   "source": [
    "## A5.\n",
    "\n",
    "_Plot the relationship between `total_bill` and `tip` to assess whether a linear trend exists._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb175f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regplot overlays a linear fit automatically; alternatively, sns.lmplot gives a facet-able interface for similar diagnostics.\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "sns.regplot(data=tips, x=\"total_bill\", y=\"tip\", scatter_kws={\"alpha\": 0.6}, ax=ax)\n",
    "ax.set_title(\"Tip vs Total Bill\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Observation: The scatter shows a clear positive linear trend between bill size and tip.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a55bd",
   "metadata": {},
   "source": [
    "## A6.\n",
    "\n",
    "_Identify the ten highest tip percentages and display their key details (bill, tip, tip_pct, day, time, size)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c59380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlargest surfaces the top rows efficiently; alternatively, tips.sort_values('tip_pct', ascending=False).head(10) is equivalent.\n",
    "top_tip_pct = tips.nlargest(10, \"tip_pct\")[\n",
    "    [\"total_bill\", \"tip\", \"tip_pct\", \"day\", \"time\", \"size\"]\n",
    "].reset_index(drop=True)\n",
    "top_tip_pct\n",
    "# Observation: Top tip percentages usually come from smaller parties with moderate bills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e3cab4",
   "metadata": {},
   "source": [
    "## A7.\n",
    "\n",
    "_Create and visualize the correlation matrix for the numeric tipping features to diagnose multicollinearity._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmaps make correlation structure interpretable; alternatively, pd.plotting.scatter_matrix compares pairwise relations numerically.\n",
    "corr = tips[[\"total_bill\", \"tip\", \"tip_pct\", \"size\"]].corr()\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", center=0, ax=ax)\n",
    "ax.set_title(\"Correlation Matrix for Tip Metrics\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Observation: total_bill and tip correlate strongly, while party size relates more weakly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f8f77",
   "metadata": {},
   "source": [
    "## A8.\n",
    "\n",
    "_Test whether dinner tip percentages differ from lunch tip percentages using an appropriate statistical test._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welch's t-test is robust to unequal variances; alternatively, statsmodels.stats.weightstats.ttest_ind handles similar comparisons with extra output.\n",
    "from scipy import stats\n",
    "\n",
    "dinner_tip_pct = tips.loc[tips[\"time\"] == \"Dinner\", \"tip_pct\"].dropna()\n",
    "lunch_tip_pct = tips.loc[tips[\"time\"] == \"Lunch\", \"tip_pct\"].dropna()\n",
    "\n",
    "ttest_res = stats.ttest_ind(dinner_tip_pct, lunch_tip_pct, equal_var=False)\n",
    "print(f\"Statistic: {ttest_res.statistic:.3f}, p-value: {ttest_res.pvalue:.4f}\")\n",
    "pd.Series({\n",
    "    \"Dinner mean tip %\": dinner_tip_pct.mean(),\n",
    "    \"Lunch mean tip %\": lunch_tip_pct.mean()\n",
    "}).round(3)\n",
    "# Observation: The p-value typically falls below 0.05, indicating dinner tips exceed lunch tips.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79751247",
   "metadata": {},
   "source": [
    "## A9.\n",
    "\n",
    "_Train a linear regression model to predict `tip` from bill size, party size, and categorical context (sex, smoker, day, time). Report RMSE and R² on a hold-out set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2388025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer keeps preprocessing reproducible; alternatively, statsmodels.OLS offers built-in categorical handling via patsy formulas.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "feature_cols = [\"total_bill\", \"size\", \"sex\", \"smoker\", \"day\", \"time\"]\n",
    "X = tips[feature_cols]\n",
    "y = tips[\"tip\"]\n",
    "\n",
    "numeric_features = [\"total_bill\", \"size\"]\n",
    "categorical_features = [\"sex\", \"smoker\", \"day\", \"time\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "linreg_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"model\", LinearRegression())\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "linreg_pipeline.fit(X_train, y_train)\n",
    "y_pred = linreg_pipeline.predict(X_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R^2: {r2:.3f}\")\n",
    "# Observation: RMSE stays around a dollar and R² is moderate, so the linear model captures broad patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c14f99",
   "metadata": {},
   "source": [
    "## A10.\n",
    "\n",
    "_Diagnose the regression model by visualizing residuals and checking for systematic patterns._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots help spot heteroskedasticity; alternatively, sns.residplot provides a quick diagnostic for linear fits.\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(residuals, kde=True, ax=axes[0])\n",
    "axes[0].set_title(\"Residual Distribution\")\n",
    "axes[0].set_xlabel(\"Residual\")\n",
    "\n",
    "sns.scatterplot(x=y_pred, y=residuals, ax=axes[1])\n",
    "axes[1].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "axes[1].set_xlabel(\"Predicted tip\")\n",
    "axes[1].set_ylabel(\"Residual\")\n",
    "axes[1].set_title(\"Predicted vs Residual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Observation: Residuals center near zero with mild heteroskedasticity at higher predicted tips.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d4813",
   "metadata": {},
   "source": [
    "## A11 – Goal: Multivariate Profiling.\n",
    "\n",
    "_Load the `penguins` dataset and quantify missing values per column._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50925ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_counts or DataFrame.info() provide similar checks; here we keep it tabular for clarity.\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "missing_counts = penguins.isna().sum().rename(\"missing\")\n",
    "missing_counts\n",
    "# Observation: Missingness concentrates in sex and bill length measurements for penguins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b418b2",
   "metadata": {},
   "source": [
    "## A12.\n",
    "\n",
    "_Impute missing numeric values in `penguins` with the median and confirm that imputed columns are now complete._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ebf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleImputer centralizes the fill strategy; alternatively, penguins.fillna(penguins.median()) is a one-liner but hides fitted state.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "penguins_clean = penguins.copy()\n",
    "numeric_cols_penguins = penguins_clean.select_dtypes(include=\"number\").columns\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "penguins_clean[numeric_cols_penguins] = imputer.fit_transform(penguins_clean[numeric_cols_penguins])\n",
    "\n",
    "penguins_clean[numeric_cols_penguins].isna().sum()\n",
    "# Observation: Post-imputation, all numeric penguin columns report zero missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc0a9d",
   "metadata": {},
   "source": [
    "## A13.\n",
    "\n",
    "_Produce a species-level summary table showing mean numeric measurements, ordered by flipper length._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby + mean highlights differences; alternatively, penguins_clean.pivot_table(index='species', values=numeric_cols_penguins, aggfunc='mean') yields the same view.\n",
    "species_summary = (\n",
    "    penguins_clean.groupby(\"species\")[numeric_cols_penguins]\n",
    "    .mean()\n",
    "    .sort_values(\"flipper_length_mm\", ascending=False)\n",
    "    .round(1)\n",
    ")\n",
    "species_summary\n",
    "# Observation: Gentoo penguins lead in flipper length, consistent with their larger body mass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ada3a",
   "metadata": {},
   "source": [
    "## A14.\n",
    "\n",
    "_Use a pair plot to compare bill and flipper measurements across species._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63789e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot offers a quick multivariate view; alternatively, sns.scatterplot with hue can be composed per pair for more control.\n",
    "sns.pairplot(\n",
    "    data=penguins_clean,\n",
    "    vars=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"],\n",
    "    hue=\"species\",\n",
    "    corner=True,\n",
    "    plot_kws={\"alpha\": 0.7}\n",
    ")\n",
    "plt.show()\n",
    "# Observation: Pair plots reveal clear species separation by bill depth and flipper length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b3f9ef",
   "metadata": {},
   "source": [
    "## A15.\n",
    "\n",
    "_Train a multinomial logistic regression model to classify penguin species using numeric and categorical features; evaluate with 5-fold cross-validation accuracy._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c896b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pipeline keeps preprocessing and modeling together; alternatively, RandomForestClassifier can handle categorical data with minimal scaling.\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "numeric_penguins = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "categorical_penguins = [\"island\", \"sex\"]\n",
    "\n",
    "penguins_features = penguins_clean[numeric_penguins + categorical_penguins]\n",
    "penguins_target = penguins_clean[\"species\"]\n",
    "\n",
    "penguins_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"scaler\", StandardScaler())]), numeric_penguins),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_penguins),\n",
    "    ]\n",
    ")\n",
    "\n",
    "log_reg_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"pre\", penguins_preprocessor),\n",
    "        (\"clf\", LogisticRegression(max_iter=200, multi_class=\"multinomial\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "cv_scores = cross_val_score(log_reg_pipeline, penguins_features, penguins_target, cv=5, scoring=\"accuracy\")\n",
    "print(f\"CV accuracy mean: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "# Observation: Cross-validated accuracy typically lands above 0.95 for this feature set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686a9ab",
   "metadata": {},
   "source": [
    "## A16 – Goal: Applied Modeling.\n",
    "\n",
    "_Load the `mpg` dataset and compute average miles-per-gallon by car origin and cylinder count._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46361f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot_table again gives a neat matrix; alternatively, mpg.groupby(['origin','cylinders'])['mpg'].mean().unstack() works fine.\n",
    "mpg = sns.load_dataset(\"mpg\").dropna(subset=[\"mpg\"])\n",
    "mpg_pivot = mpg.pivot_table(values=\"mpg\", index=\"origin\", columns=\"cylinders\", aggfunc=\"mean\").round(1)\n",
    "mpg_pivot\n",
    "# Observation: MPG drops as cylinder count increases, especially for U.S. origin vehicles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ebd176",
   "metadata": {},
   "source": [
    "## A17.\n",
    "\n",
    "_Evaluate whether cylinder count and origin are independent using a chi-squared test._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-squared via scipy is standard; alternatively, statsmodels.stats.contingency_tables.Table conducts the same test with extra helpers.\n",
    "contingency_table = pd.crosstab(mpg[\"origin\"], mpg[\"cylinders\"])\n",
    "chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-squared: {chi2_stat:.2f}, p-value: {p_value:.4f}, dof: {dof}\")\n",
    "pd.DataFrame(expected, index=contingency_table.index, columns=contingency_table.columns).round(1)\n",
    "# Observation: A low p-value suggests cylinder count and origin are not independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb0a2f",
   "metadata": {},
   "source": [
    "## A18.\n",
    "\n",
    "_Build a random forest regression pipeline (with appropriate preprocessing) to predict `mpg`; report RMSE and R² on a hold-out set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a407e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests handle nonlinearities well; alternatively, GradientBoostingRegressor is a strong baseline with fewer trees.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "feature_cols_mpg = mpg.drop(columns=[\"name\", \"mpg\"])\n",
    "X_mpg = feature_cols_mpg\n",
    "y_mpg = mpg[\"mpg\"]\n",
    "\n",
    "numeric_mpg = X_mpg.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_mpg = X_mpg.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "mpg_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), numeric_mpg),\n",
    "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), categorical_mpg),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"pre\", mpg_preprocessor),\n",
    "        (\"model\", RandomForestRegressor(n_estimators=200, random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_mpg, X_test_mpg, y_train_mpg, y_test_mpg = train_test_split(\n",
    "    X_mpg, y_mpg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "rf_pipeline.fit(X_train_mpg, y_train_mpg)\n",
    "rf_preds = rf_pipeline.predict(X_test_mpg)\n",
    "\n",
    "rf_rmse = mean_squared_error(y_test_mpg, rf_preds, squared=False)\n",
    "rf_r2 = r2_score(y_test_mpg, rf_preds)\n",
    "\n",
    "print(f\"RandomForest RMSE: {rf_rmse:.2f}\")\n",
    "print(f\"RandomForest R^2: {rf_r2:.3f}\")\n",
    "# Observation: Random forest RMSE hovers around 2–3 MPG with an R² near 0.85.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309b0c8",
   "metadata": {},
   "source": [
    "## A19.\n",
    "\n",
    "_Estimate permutation feature importances for the random forest model on the test data and list the top drivers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance measures predictive impact; alternatively, rf_pipeline.named_steps['model'].feature_importances_ offers tree-based importances.\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_result = permutation_importance(\n",
    "    rf_pipeline,\n",
    "    X_test_mpg,\n",
    "    y_test_mpg,\n",
    "    n_repeats=20,\n",
    "    random_state=42,\n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "\n",
    "feature_names = rf_pipeline.named_steps[\"pre\"].get_feature_names_out()\n",
    "importance_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": perm_result.importances_mean\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "importance_df.head(10)\n",
    "# Observation: Displacement, horsepower, and weight usually rank as the top drivers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cad0e9",
   "metadata": {},
   "source": [
    "## A20.\n",
    "\n",
    "_Plot predicted versus actual `mpg` for the test set and report mean absolute error to judge calibration._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter + baseline line visualizes calibration; alternatively, sns.residplot examines residuals directly against predictions.\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Actual MPG\": y_test_mpg,\n",
    "    \"Predicted MPG\": rf_preds\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.scatterplot(data=comparison_df, x=\"Actual MPG\", y=\"Predicted MPG\", ax=ax, alpha=0.7)\n",
    "min_val, max_val = comparison_df.min().min(), comparison_df.max().max()\n",
    "ax.plot([min_val, max_val], [min_val, max_val], linestyle=\"--\", color=\"red\")\n",
    "ax.set_title(\"Random Forest Predictions vs Actual MPG\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mae = mean_absolute_error(y_test_mpg, rf_preds)\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "# Observation: Predictions track actual MPG closely with MAE near 2 MPG.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leet-ish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
